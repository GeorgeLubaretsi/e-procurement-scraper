#! /usr/bin/env python
#encoding:UTF-8


from scrapy.selector import HtmlXPathSelector
from scrapy.spider import BaseSpider
from scrapy.http import Request
from procurementScrape.items import Tender, Organisation, TenderBidder, TenderAgreement
from time import sleep
import httplib2

class ProcurementSpider(BaseSpider):
    name = "procurement"
    allowed_domains = ["procurement.gov.ge", "tenders.procurement.gov.ge"]
    mainPageBaseUrl = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=search_app&page="
    start_urls = [self.mainPageBaseUrl+"1"]
    tenderCount = 0
    
    def setSessionCookie(self,sessionCookie):
        self.sessionCookie = sessionCookie
    
    
    def parseResultsPage(self,response):
        #print "parsing results"
        hxs = HtmlXPathSelector(response)
        resultsDividers = hxs.select('//div[contains(@id, "agency_docs")]//div').extract()
        if resultsDividers.__len__() >= 3:
            winnerDiv = resultsDividers[2]
            
            #check for disqualifications
            if winnerDiv.find("დისკვალიფიკაცია") > -1 :
                return
            
            item = TenderAgreement()
            item["tenderID"] = response.meta['tenderID']
            
            index = winnerDiv.find("ShowProfile")
            index = winnerDiv.find("(",index)
            endIndex = winnerDiv.find(")",index)
            item["OrgUrl"] = winnerDiv[index+1:endIndex]
            
            index = winnerDiv.find("ნომერი/თანხა:")
            index = winnerDiv.find(":",index)
            endIndex = winnerDiv.find("<",index)  
            item["Amount"] = winnerDiv[index+2:endIndex]
            
            index = winnerDiv.find("ძალაშია",index)
            index = winnerDiv.find(":",index)
            endIndex = winnerDiv.find("-",index)
            item["StartDate"] = winnerDiv[index+1:endIndex]
            
            index = endIndex
            endIndex = winnerDiv.find("<",index)
            item["ExpiryDate"] = winnerDiv[index+1:endIndex]
            
            yield item
    
    def parseBidsPage(self,response):
        #print "parsing bids"
        hxs = HtmlXPathSelector(response)
        bidRows = hxs.select('//div[contains(@id, "app_bids")]//table[last()]/tbody//tr').extract()
        if bidRows.__len__() == 0:
            return
        for bidder in bidRows:        
            item = TenderBidder()
            item["tenderID"] = response.meta['tenderID']
            
            index = bidder.find("ShowProfile")
            index = bidder.find("(",index)
            endIndex = bidder.find(")",index)
            item["OrgUrl"] = bidder[index+1:endIndex]
    
            index = bidder.find("strong")
            index = bidder.find(">",index)
            endIndex = bidder.find("<",index)
            item["firstBidAmount"] = bidder[index+1:endIndex]
            
            index = bidder.find("date",index)
            index = bidder.find(">",index)
            endIndex = bidder.find("<",index)
            item["firstBidDate"] = bidder[index+1:endIndex]
            
            index = bidder.find("activebid1",index)
            index = bidder.find(">",index)
            endIndex = bidder.find("<",index)
            item["lastBidAmount"] = bidder[index+1:endIndex]
            
            index = bidder.find("date",index)
            index = bidder.find(">",index)
            endIndex = bidder.find("<",index)
            item["lastBidDate"] = bidder[index+1:endIndex]
            yield item
            
            #now lets use the company id to scrape the company data
            url = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=profile&org_id="+item['OrgUrl']
            organisation_request = Request(url, callback=self.parseOrganisation, cookies={"SPALITE":self.sessionCookie})
            organisation_request.headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US))')     
            organisation_request.meta['OrgUrl'] = item['OrgUrl']
            organisation_request.meta['type'] = "biddingOrg"
            yield organisation_request
    
    def parseOrganisation(self,response):
        #print "parsing procurer"
        hxs = HtmlXPathSelector(response)
        keyPairs = hxs.select('//div[contains(@id, "profile_dialog")]//tr').extract()
        item = Organisation()
        item["OrgUrl"] = response.meta['OrgUrl']

        index = keyPairs[0].find("label")
        index = keyPairs[0].find(">",index)
        endIndex = keyPairs[0].find("<",index)
        item["Name"] = keyPairs[0][index+1:endIndex]
        
        index = keyPairs[1].find("/td")
        index = keyPairs[1].find("<td",index)
        index = keyPairs[1].find(">",index)
        endIndex = keyPairs[1].find("<",index)
        item["OrgID"] = keyPairs[1][index+1:endIndex]
        
        index = keyPairs[2].find("/td")
        index = keyPairs[2].find("<td",index)
        index = keyPairs[2].find(">",index)
        endIndex = keyPairs[2].find("<",index)
        item["Country"] = keyPairs[2][index+1:endIndex]
        item["Type"] = response.meta['type']
        
        #[0] = entity#[1] = id #[2] = country #[3] = city  #[4] = address    #[5] = phone    #6 = fax #7 email #8 website #9contact
        yield item
    
    def parseTender(self, response):
        self.tenderCount = self.tenderCount + 1
        print "parsing Tender: "+str(self.tenderCount)
        hxs = HtmlXPathSelector(response)
        keyPairs = hxs.select('//tr/td').extract()  
        item = Tender()
   
        item['tenderID'] =  response.meta['tenderUrl']
        
        index = keyPairs[7].find("ShowProfile")
        index = keyPairs[7].find("(",index)
        endIndex = keyPairs[7].find(")",index)
        item['procuringEntityUrl'] = keyPairs[7][index+1:endIndex]
        
        index = keyPairs[7].find("<img")
        index = keyPairs[7].find(">", index)
        endIndex = keyPairs[7].find("</",index)
        item['procuringEntityName'] = keyPairs[7][index+1:endIndex]
        
        index = keyPairs[1].find(">")
        endIndex = keyPairs[1].find("<",index)
        item['tenderType'] = keyPairs[1][index+1:endIndex]
        
        index = keyPairs[3].find("strong")
        index = keyPairs[3].find(">",index)
        endIndex = keyPairs[3].find("<",index)
        item['tenderRegistrationNumber'] = keyPairs[3][index+1:endIndex]

        index = keyPairs[5].find("img")
        index = keyPairs[5].find(">",index)
        endIndex = keyPairs[5].find("<",index)
        item['tenderStatus'] = keyPairs[5][index+1:endIndex]
        
        index = keyPairs[9].find(">")
        endIndex = keyPairs[9].find("<",index)
        item['tenderAnnouncementDate'] = keyPairs[9][index+1:endIndex]
        
        index = keyPairs[11].find(">")
        endIndex = keyPairs[11].find("<",index)
        item['bidsStartDate'] = keyPairs[11][index+1:endIndex]
        
        index = keyPairs[11].find(">")
        endIndex = keyPairs[11].find("<",index)
        item['bidsEndDate'] = keyPairs[13][index+1:endIndex]
        
        index = keyPairs[15].find("span")
        index = keyPairs[15].find(">",index)
        endIndex = keyPairs[15].find("<",index)
        item['estimatedValue'] = keyPairs[15][index+1:endIndex]
        
        #item['includeVat']
        index = keyPairs[19].find("/strong")
        index = keyPairs[19].find(">",index)
        endIndex = keyPairs[19].find("<",index)
        item['cpvCode'] = keyPairs[19][index+1:endIndex]
    
        #now lets use the procuring entity id to find more info about the procurer
        url = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=profile&org_id="+item['procuringEntityUrl']
        procurer_request = Request(url, callback=self.parseOrganisation, cookies={"SPALITE":self.sessionCookie})
        procurer_request.headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US))')     
        procurer_request.meta['OrgUrl'] = item['procuringEntityUrl']
        procurer_request.meta['type'] = "procuringOrg"
        #yield procurer_request
        
        #now lets look at the bids made on this tender
        url = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=app_bids&app_id="+item['tenderID']
        bids_request = Request(url, callback=self.parseBidsPage, cookies={"SPALITE":self.sessionCookie})
        bids_request.headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US))')  
        bids_request.meta['tenderID'] = item['tenderID']
        #yield bids_request
        
        #finally lets look at the results of this tender
        url = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=agency_docs&app_id="+item['tenderID']
        results_request = Request(url, callback=self.parseResultsPage,cookies={"SPALITE":self.sessionCookie})
        results_request.headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US))')  
        results_request.meta['tenderID'] = item['tenderID']
        #yield results_request
        
        return item, results_request, procurer_request, bids_request
        
    def parseTenderUrls(self, response):
        hxs = HtmlXPathSelector(response)
        tenderOnClickItems = hxs.select('//table[@id="list_apps_by_subject"]//tr//@onclick').extract()

        for tenderOnClickItem in tenderOnClickItems:
            base_tender_url = "https://tenders.procurement.gov.ge/public/lib/controller.php?action=app_main&app_id="
            index = tenderOnClickItem.find("ShowApp")
            index = tenderOnClickItem.find("(",index)
            endIndex = tenderOnClickItem.find(",",index)
            index_url = tender_url[index+1:endIndex]
            tender_url = base_tender_url+index_url
            print "parsing"+tender_url
            request = Request(tender_url, callback=self.parseTender, cookies={"SPALITE":self.sessionCookie})
            request.headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US))')
            request.meta['tenderUrl'] = index_url
            
            yield request
    
    def parse(self, response):
        #Find index of last page
        hxs = HtmlXPathSelector(response)

        totalPagesButton = hxs.select('//div[@class="pager pad4px"]//button').extract()[2]
        
        index = totalPagesButton.find('/')
        endIndex = totalPagesButton.find(')')
        final_page = totalPagesButton[index+1:endIndex]
        
        if( final_page == -1 ):
            #print "Parsing Error... stopping"
            return
        
        print "Starting scrape"  
        for i in range(1,int(final_page)+1):
            url = self.mainPageBaseUrl+str(i)
            request = Request(url, callback=self.parseTenderUrls, cookies={"SPALITE":self.sessionCookie})
            print "parsing page: "+ str(i)
            yield request


def main():
    from scrapy import signals
    from scrapy.xlib.pydispatch import dispatcher
 
    # shut off log
    from scrapy.conf import settings
    settings.overrides['LOG_ENABLED'] = False
 
    # set up crawler
    from scrapy.crawler import CrawlerProcess
 
    crawler = CrawlerProcess(settings)
    crawler.install()
    crawler.configure()
 
    #first get cookie from dummy request
    http = httplib2.Http()
    url = "https://tenders.procurement.gov.ge/public/?go=1000"
    headers={"User-Agent":'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; en-US'}
    response, content = http.request(url, 'POST', headers=headers)
    if( response['set-cookie'] ):
        cookieString = response['set-cookie']
        index = cookieString.find('SPALITE')
        index = cookieString.find("=",index)
        endIndex = cookieString.find(';',index)
        spaLite = cookieString[index+1:endIndex]

 
    # schedule spider
    procurementSpider = ProcurementSpider()
    procurementSpider.setSessionCookie(spaLite)
    crawler.crawl(procurementSpider)

    #start engine scrapy/twisted
    print "STARTING ENGINE"
    crawler.start()
    print "ENGINE STOPPED"
 
if __name__ == '__main__':
    main()
        
        
